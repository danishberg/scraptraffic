#!/usr/bin/env python3
"""
phone_agent.py â€“ Scraptraffic voice bot for OpenAI Realtime API
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Lists mics with --list-devices, choose one with --mic N  
â€¢ WebRTC-VAD if installed; pure-Python energy VAD otherwise  
â€¢ Starts recording after â‰¥200 ms of speech; needs â‰¥400 ms total to send  
â€¢ Sends utterance when â‰¥2 s of silence is detected  
â€¢ Wonâ€™t start a new turn until the assistant finishes the previous one  
â€¢ Press â€œqâ€ in the console to quit
"""

from __future__ import annotations
import argparse, asyncio, os, sys, time
from typing import Optional

import numpy as np
import sounddevice as sd
from pynput import keyboard
from openai_realtime_client import RealtimeClient, AudioHandler, TurnDetectionMode

# â”€â”€ Optional WebRTC-VAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import webrtcvad  # type: ignore
    HAVE_WEBRTC = True
except ModuleNotFoundError:
    HAVE_WEBRTC = False

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAMPLE_RATE              = 16_000
FRAME_MS                 = 30
FRAME_SAMPLES            = SAMPLE_RATE * FRAME_MS // 1_000
START_SPEECH_MS          = 200
START_SPEECH_FRAMES      = START_SPEECH_MS // FRAME_MS
END_SILENCE_MS           = 2_000
END_SILENCE_FRAMES       = END_SILENCE_MS // FRAME_MS
MIN_UTTERANCE_MS         = 400         # ignore <400 ms blips
MIN_UTTERANCE_FRAMES     = MIN_UTTERANCE_MS // FRAME_MS
CALIBRATE_SEC            = 1.5
ENERGY_MULT              = 4.0
ENERGY_OFFSET            = 0.003

SYSTEM_PROMPT = """
Ğ¢Ğ²Ğ¾Ñ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ° - ÑÑ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸ĞµĞ¼Ñƒ Ğ·Ğ²Ğ¾Ğ½ĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Scraptraffic, Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰ÑƒÑÑÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ¾Ğ»Ğ¾Ğ¼Ğ°. Ğ¡ĞºĞ¾Ñ€ĞµĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ·Ğ²Ğ¾Ğ½Ğ¾Ğº Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ñ‚ Ñ‚Ğ¾Ğ³Ğ¾, ĞºÑ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑ‚ ÑĞ´Ğ°Ñ‚ÑŒ ĞºĞ°ĞºĞ¾Ğ¹-Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ». Ğ’ ÑÑ‚Ğ¾Ğ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ñ‚ĞµĞ±Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ñ‚ÑŒ: Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° - Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ³Ğ¾Ñ€Ğ¾Ğ´. . \
Ğ¢ĞµĞ±Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ‹ÑÑĞ½Ğ¸Ñ‚ÑŒ ĞºĞ°ĞºĞ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ» Ñ…Ğ¾Ñ‚ÑÑ‚ ÑĞ´Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ»Ğ¾Ğ¼, ĞºĞ°ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ğ¸, Ğ² ĞºĞ°ĞºĞ¾Ğ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ», Ğ¸ ĞµÑÑ‚ÑŒ Ğ»Ğ¸ ĞºĞ°ĞºĞ¸Ğµ-Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ğ¸.\
Ğ’ ĞºĞ¾Ğ½Ñ†Ğµ ÑĞ¿Ñ€Ğ¾ÑĞ¸ - Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ñ‹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ°Ğ¼ Ğ½Ğ°ÑˆĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞµÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ ĞµĞ¼Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ ÑĞ´ĞµĞ»ĞºĞ¸ Ğ¸ Ñ†ĞµĞ½Ñƒ.

Ğ•ÑĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ½Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ñ‹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ·Ğ°Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ·Ğ° ĞµĞ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ», Ñ‚Ğ¾ Ğ²ĞµĞ¶Ğ»Ğ¸Ğ²Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ñ†ĞµĞ½Ñƒ ÑĞ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€, Ğ° Ñ‚Ñ‹ ÑĞ²Ğ»ÑĞµÑˆÑŒÑÑ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°ÑĞ²ĞºÑƒ, Ğ¸ Ñ‚Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¾Ğ¾Ğ±Ñ‰Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ, Ñ‡Ñ‚Ğ¾ Ñ‡Ñ‚Ğ¾ Ğ² Ğ½Ğ°ÑˆĞµĞ¹ ÑĞµÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑĞºĞ¸Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ°, ĞµÑĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ´Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸Ğµ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ÑĞ²ĞºĞ¸ Ğ½Ğ°ÑˆĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ°Ğ¼, Ñ‚Ğ¾ Ğ¾Ñ‚ Ğ½Ğ°Ğ²ĞµÑ€Ğ½ÑĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ Ğ¾Ñ‚ Ğ½Ğ¸Ñ… Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ.

Ğ•ÑĞ»Ğ¸ Ñ‚Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑˆÑŒ, Ñ‡Ñ‚Ğ¾ Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ½Ğµ ĞºĞ°ÑĞ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ° Ğ»Ğ¾Ğ¼Ğ°, Ğ²ĞµĞ¶Ğ»Ğ¸Ğ²Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ ĞµĞ¼Ñƒ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ²Ğ¾Ñ Ñ€Ğ¾Ğ»ÑŒ - Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ·Ğ°ÑĞ²ĞºĞ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ¼Ğ°, Ğ¸ Ğ½Ğµ Ğ½Ğ° ĞºĞ°ĞºĞ¸Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑˆÑŒ.

ĞšĞ°Ğº Ğ±Ñ‹ Ñ‚ĞµĞ±Ñ Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° ĞºĞ°ĞºĞ¸Ğµ-Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ½Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑˆĞ°Ğ¹ÑÑ ÑÑ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ.\
ĞŸĞ¾ÑĞ»Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ñ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ñ‰Ğ°Ğ»Ğ¸ÑÑŒ ÑĞµĞ°Ğ½Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ²ÑĞ·Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡ĞµĞ½.

ĞŸĞ¾ Ğ¸Ñ‚Ğ¾Ğ³Ñƒ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞ¹ Ğ·Ğ°ÑĞ²ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.
""".strip()

# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def play_audio_safe(handler: AudioHandler, pcm: bytes) -> None:
    try:
        handler.play_audio(pcm)
    except Exception:
        pass

class Flag:
    """Tiny thread-safe boolean container."""
    def __init__(self) -> None: self._state = False
    def set(self, v: bool) -> None: self._state = v
    def get(self) -> bool: return self._state

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main() -> None:
    # CLI --------------------------------------------------------------------
    p = argparse.ArgumentParser()
    p.add_argument("--list-devices", action="store_true")
    p.add_argument("--mic", type=int, help="index returned by --list-devices")
    p.add_argument("--debug-level", choices=("none", "energy"), default="none")
    args = p.parse_args()

    # Devices ----------------------------------------------------------------
    devs = sd.query_devices()
    ins = [(i, d) for i, d in enumerate(devs) if d["max_input_channels"] > 0]
    if args.list_devices:
        for i, d in ins: print(f"[{i}] {d['name']}")
        sys.exit(0)

    mic_index = (
        args.mic
        if args.mic is not None
        else next(
            i
            for i, d in ins
            if "loopback" not in d["name"].lower()
            and "stereo mix" not in d["name"].lower()
        )
    )
    print(f"âœ”ï¸  Using mic #{mic_index}: {devs[mic_index]['name']}")

    # API key -----------------------------------------------------------------
    api_key = os.getenv("OPENAI_API_KEY") or sys.exit("OPENAI_API_KEY not set")

    loop = asyncio.get_running_loop()
    speaker = AudioHandler()
    speaking = Flag()
    awaiting_response = Flag()

    # Assistant event handlers -----------------------------------------------
    def _on_audio_start(*_): speaking.set(True)
    def _on_audio_done(*_):
        speaking.set(False)
        awaiting_response.set(False)

    client = RealtimeClient(
        api_key=api_key,
        instructions=SYSTEM_PROMPT,
        voice="alloy",
        turn_detection_mode=TurnDetectionMode.MANUAL,
        on_text_delta=lambda t: print(f"\nAssistant: {t}", end="", flush=True),
        on_audio_delta=lambda p: play_audio_safe(speaker, p),
        extra_event_handlers={
            "response.audio.delta": _on_audio_start,
            "response.audio.done":  _on_audio_done,
        },
    )

    # Quit on â€œqâ€
    keyboard.Listener(
        on_press=lambda k: loop.stop() if getattr(k, "char", "") == "q" else None
    ).start()

    # Connect & greeting ------------------------------------------------------
    await client.connect()
    print("ğŸ™ï¸  Connected â€“ playing greeting â€¦")
    await client.create_response()
    asyncio.create_task(client.handle_messages())

    # VAD ---------------------------------------------------------------------
    if HAVE_WEBRTC:
        vad = webrtcvad.Vad(2)
    else:
        print("[INFO] webrtcvad not found â€“ using energy VAD")
        print(f"ğŸ”‡  Calibrating {CALIBRATE_SEC}s of ambience â€¦")
        amb = sd.rec(
            int(CALIBRATE_SEC * SAMPLE_RATE),
            samplerate=SAMPLE_RATE,
            channels=1,
            dtype="float32",
            device=mic_index,
        )
        sd.wait()
        rms = np.sqrt(np.mean(np.square(amb[:, 0])))
        energy_threshold = rms * ENERGY_MULT + ENERGY_OFFSET
        print(f"âœ…  Initial energy threshold: {energy_threshold:.6f}")

    # State -------------------------------------------------------------------
    in_speech = False
    voice_frames = silence_frames = 0
    buffer = bytearray()
    last_voice: Optional[float] = None
    session_start = time.time()

    # Audio callback ----------------------------------------------------------
    def cb(indata, *_):
        nonlocal in_speech, voice_frames, silence_frames, buffer, energy_threshold, last_voice

        # Ignore while assistant is speaking or response is pending
        if speaking.get() or awaiting_response.get():
            return

        pcm16 = (indata[:, 0] * 32767).astype(np.int16).tobytes()
        if HAVE_WEBRTC:
            is_voice = vad.is_speech(pcm16, SAMPLE_RATE)
        else:
            energy = np.sqrt(np.mean(indata[:, 0] ** 2))
            is_voice = energy > energy_threshold
            if args.debug_level == "energy":
                print(f"\rEnergy {energy:.4f} thr {energy_threshold:.4f}", end="")

        # Track speech / silence ---------------------------------
        if is_voice:
            voice_frames += 1
            silence_frames = 0
            last_voice = time.time()
        else:
            if in_speech:
                silence_frames += 1
            voice_frames = 0

        # Start utterance ----------------------------------------
        if not in_speech and voice_frames >= START_SPEECH_FRAMES:
            in_speech = True
            buffer.clear()
            print("\n[ğŸ¤  Recording â€¦]")

        # Buffer audio -------------------------------------------
        if in_speech:
            buffer.extend(pcm16)

        # End utterance ------------------------------------------
        if in_speech and silence_frames >= END_SILENCE_FRAMES:
            in_speech = False
            silence_frames = 0
            frames_in_utt = len(buffer) // 2 // FRAME_SAMPLES
            if frames_in_utt >= MIN_UTTERANCE_FRAMES:
                print("[ğŸ“¤  Sending utterance]")
                asyncio.run_coroutine_threadsafe(
                    client.stream_audio(bytes(buffer)), loop
                )
                awaiting_response.set(True)
                asyncio.run_coroutine_threadsafe(client.create_response(), loop)
            buffer.clear()

        # Auto-adjust threshold (energy VAD only) ----------------
        if (
            not HAVE_WEBRTC
            and last_voice is None
            and time.time() - session_start > 8
        ):
            energy_threshold *= 0.7
            last_voice = time.time()
            print(f"\n[INFO] Lowering threshold â†’ {energy_threshold:.4f}")

    # Mic stream --------------------------------------------------------------
    with sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype="float32",
        blocksize=FRAME_SAMPLES,
        device=mic_index,
        callback=cb,
    ):
        print("ğŸ”´  Speak when ready (press q to quit) â€¦")
        try:
            await asyncio.Future()
        except asyncio.CancelledError:
            pass

    # Cleanup -----------------------------------------------------------------
    speaker.cleanup()
    await client.close()
    print("\nğŸ‘‹  Bye")


# Entrypoint ------------------------------------------------------------------
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        pass
