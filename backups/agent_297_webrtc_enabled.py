#!/usr/bin/env python3
"""
phone_agent.py â€“ Scraptraffic voice bot for OpenAI Realtime API
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Lists mics with --list-devices, choose one with --mic N
â€¢ Lists speakers with --list-output-devices, choose one with --output-device N
â€¢ WebRTC-VAD if installed; pure-Python energy VAD otherwise
â€¢ Starts recording after â‰¥100 ms of consecutive speech
â€¢ Sends utterance when â‰¥2000 ms of continuous silence is detected
â€¢ Wonâ€™t start a new turn until the assistant finishes the previous one
â€¢ Press â€œqâ€ in the console to quit
â€¢ Bot will finish speaking entirely before listening again
â€¢ All audio played live via a RawOutputStream
â€¢ Displays a single-line â€œmic statusâ€ indicator (Voice vs Silence)
â€¢ Prints â€œðŸ‘‚ Your turn â€” listening nowâ€ whenever the assistant is done
"""

from __future__ import annotations
import argparse
import asyncio
import os
import sys
import time

import numpy as np
import sounddevice as sd
from pynput import keyboard

from openai_realtime_client import RealtimeClient, TurnDetectionMode

# â”€â”€ Optional WebRTC-VAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import webrtcvad  # type: ignore
    HAVE_WEBRTC = True
except ModuleNotFoundError:
    HAVE_WEBRTC = False

# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SAMPLE_RATE            = 16_000
FRAME_MS               = 30
FRAME_SAMPLES          = SAMPLE_RATE * FRAME_MS // 1_000

# how many frames of speech to start (100 ms â†’ ~4 frames)
START_SPEECH_FRAMES    = 100 // FRAME_MS  
# how many total frames minimum to accept an utterance (400 ms â†’ ~13 frames)
MIN_UTTERANCE_FRAMES   = 400 // FRAME_MS  
# how many frames of silence to end (2000 ms â†’ ~66 frames)
END_SILENCE_FRAMES     = 2000 // FRAME_MS  

CALIBRATE_SEC          = 1.5
ENERGY_MULT            = 4.0
ENERGY_OFFSET          = 0.003

# Realtime API emits 24 kHz 16-bit PCM
OUTPUT_SR              = 24_000

SYSTEM_PROMPT = """
Ð¢Ð²Ð¾Ñ Ñ€Ð¾Ð»ÑŒ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ° - ÑÑ‚Ð¾ Ð¾Ð¿ÐµÑ€Ð°Ñ‚Ð¾Ñ€ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ð½Ð¾Ð¹ Ð»Ð¸Ð½Ð¸Ð¸ Ð¿Ð¾ Ð¿Ñ€Ð¸ÐµÐ¼Ñƒ Ð·Ð²Ð¾Ð½ÐºÐ¾Ð² Ð² ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÑŽ Scraptraffic, Ð·Ð°Ð½Ð¸Ð¼Ð°ÑŽÑ‰ÑƒÑŽÑÑ Ð¿Ñ€Ð¸ÐµÐ¼Ð¾Ð¼ Ð¼ÐµÑ‚Ð°Ð»Ð»Ð¾Ð»Ð¾Ð¼Ð°. Ð¡ÐºÐ¾Ñ€ÐµÐµ Ð²ÑÐµÐ³Ð¾ Ð²Ñ…Ð¾Ð´ÑÑ‰Ð¸Ð¹ Ð·Ð²Ð¾Ð½Ð¾Ðº Ð±ÑƒÐ´ÐµÑ‚ Ð¾Ñ‚ Ñ‚Ð¾Ð³Ð¾, ÐºÑ‚Ð¾ Ñ…Ð¾Ñ‡ÐµÑ‚ ÑÐ´Ð°Ñ‚ÑŒ ÐºÐ°ÐºÐ¾Ð¹-Ñ‚Ð¾ Ð¼ÐµÑ‚Ð°Ð»Ð». Ð’ ÑÑ‚Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ Ñ‚ÐµÐ±Ðµ Ð½ÑƒÐ¶Ð½Ð¾ ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚ÑŒ: Ð¼ÐµÑÑ‚Ð¾Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð° - Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐ¸Ð¹ ÐºÑ€ÑƒÐ¿Ð½Ñ‹Ð¹ Ð³Ð¾Ñ€Ð¾Ð´. \
Ð¢ÐµÐ±Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð²Ñ‹ÑÑÐ½Ð¸Ñ‚ÑŒ ÐºÐ°ÐºÐ¾Ð¹ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ñ…Ð¾Ñ‚ÑÑ‚ ÑÐ´Ð°Ñ‚ÑŒ Ð½Ð° Ð»Ð¾Ð¼, ÐºÐ°ÐºÐ¾Ð¹ Ð¾Ð±ÑŠÐµÐ¼ Ð¿Ð°Ñ€Ñ‚Ð¸Ð¸, Ð² ÐºÐ°ÐºÐ¾Ð¼ Ð³Ð¾Ñ€Ð¾Ð´Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð», Ð¸ ÐµÑÑ‚ÑŒ Ð»Ð¸ ÐºÐ°ÐºÐ¸Ðµ-Ñ‚Ð¾ Ð¾ÑÐ¾Ð±Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ ÑÑ‚Ð¾Ð¹ Ð¿Ð°Ñ€Ñ‚Ð¸Ð¸. \
Ð’ ÐºÐ¾Ð½Ñ†Ðµ ÑÐ¿Ñ€Ð¾ÑÐ¸ â€” Ð½Ðµ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² Ð»Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº, Ñ‡Ñ‚Ð¾ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð² Ñ‚Ð¾Ð¼ Ñ‡Ð¸ÑÐ»Ðµ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ, Ð±ÑƒÐ´ÑƒÑ‚ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ñ‹ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð°Ð¼ Ð½Ð°ÑˆÐµÐ¹ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸, Ð° Ñ‚Ð°ÐºÐ¶Ðµ ÑÐµÑ‚Ð¸ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ¼Ð¾Ð³ÑƒÑ‚ Ð¿Ð¾Ð·Ð²Ð¾Ð½Ð¸Ñ‚ÑŒ ÐµÐ¼Ñƒ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ ÑÐ´ÐµÐ»ÐºÐ¸ Ð¸ Ñ†ÐµÐ½Ñƒ.

Ð•ÑÐ»Ð¸ Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐµ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð±ÑƒÐ´ÐµÑ‚ Ð´Ð¾Ð±Ð¸Ð²Ð°Ñ‚ÑŒÑÑ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ†ÐµÐ½Ñƒ, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¼Ñ‹ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹ Ð·Ð°Ð¿Ð»Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð·Ð° ÐµÐ³Ð¾ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð», Ñ‚Ð¾ Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾ ÑÐ¾Ð¾Ð±Ñ‰Ð¸, Ñ‡Ñ‚Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ñ†ÐµÐ½Ñƒ ÑÐ¼Ð¾Ð³ÑƒÑ‚ ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð¸Ð»Ð¸ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€, Ð° Ñ‚Ñ‹ ÑÐ²Ð»ÑÐµÑˆÑŒÑÑ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ñ‹Ð¼ Ñ€Ð¾Ð±Ð¾Ñ‚Ð¾Ð¼, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐµÑ‚ Ð·Ð°ÑÐ²ÐºÑƒ, Ð¸ Ñ‚Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿ÐµÑ€Ð²Ð¸Ñ‡Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ. Ð¡Ð¾Ð¾Ð±Ñ‰Ð¸ Ñ‚Ð°ÐºÐ¶Ðµ, Ñ‡Ñ‚Ð¾ Ð² Ð½Ð°ÑˆÐµÐ¹ ÑÐµÑ‚Ð¸ Ð¼Ð½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€ÑÐºÐ¸Ñ… Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð² Ð¿Ñ€Ð¸Ñ‘Ð¼Ð°: ÐµÑÐ»Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð´Ð°ÑÑ‚ ÑÐ¾Ð³Ð»Ð°ÑÐ¸Ðµ Ð½Ð° Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ñƒ Ð·Ð°ÑÐ²ÐºÐ¸ Ð½Ð°ÑˆÐ¸Ð¼ Ð¿Ð°Ñ€Ñ‚Ð½Ñ‘Ñ€Ð°Ð¼, Ñ‚Ð¾ Ð¾Ð½ Ð½Ð°Ð²ÐµÑ€Ð½ÑÐºÐ° Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ Ð¾Ñ‚ Ð½Ð¸Ñ… Ð²Ñ‹Ð³Ð¾Ð´Ð½Ð¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ.

Ð•ÑÐ»Ð¸ Ñ‚Ñ‹ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑˆÑŒ, Ñ‡Ñ‚Ð¾ Ñƒ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ° Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð½Ðµ ÐºÐ°ÑÐ°ÑŽÑ‰Ð¸Ð¹ÑÑ Ð¿Ñ€Ð¸ÐµÐ¼Ð° Ð»Ð¾Ð¼Ð°, Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸ ÐµÐ¼Ñƒ, Ñ‡Ñ‚Ð¾ Ñ‚Ð²Ð¾Ñ Ñ€Ð¾Ð»ÑŒ â€” Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð·Ð°ÑÐ²ÐºÐ¸ Ð½Ð° Ð¿Ñ€Ð¸ÐµÐ¼ Ð»Ð¾Ð¼Ð°, Ð¸ Ð½Ð° ÐºÐ°ÐºÐ¸Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ñ‚Ñ‹ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ.

ÐšÐ°Ðº Ð±Ñ‹ Ñ‚ÐµÐ±Ñ Ð½Ð¸ Ð¿Ñ€Ð¾ÑÐ¸Ð»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° ÐºÐ°ÐºÐ¸Ðµ-Ñ‚Ð¾ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ð½Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð½Ðµ ÑÐ¾Ð³Ð»Ð°ÑˆÐ°Ð¹ÑÑ ÑÑ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ. \
ÐŸÐ¾ÑÐ»Ðµ Ñ‚Ð¾Ð³Ð¾, ÐºÐ°Ðº Ñ Ñ‚Ð¾Ð±Ð¾Ð¹ Ð¿Ð¾Ð¿Ñ€Ð¾Ñ‰Ð°Ð»Ð¸ÑÑŒ, ÑÐµÐ°Ð½Ñ Ð°ÑƒÐ´Ð¸Ð¾-ÑÐ²ÑÐ·Ð¸ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑ‚ÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ñ‹Ð¼.

ÐŸÐ¾ Ð¸Ñ‚Ð¾Ð³Ñƒ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° ÑÑ„Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐ¹ Ð·Ð°ÑÐ²ÐºÑƒ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼.
""".strip()

class Flag:
    """Tiny thread-safe boolean."""
    def __init__(self) -> None:
        self._v = False
    def set(self, v: bool) -> None:
        self._v = v
    def get(self) -> bool:
        return self._v

async def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--list-devices", action="store_true",
                        help="List available input (mic) devices")
    parser.add_argument("--list-output-devices", action="store_true",
                        help="List available output (speaker) devices")
    parser.add_argument("--mic", type=int,
                        help="Index of input device from --list-devices")
    parser.add_argument("--output-device", type=int,
                        help="Index of output device from --list-output-devices")
    parser.add_argument("--debug-level", choices=("none","energy"),
                        default="energy", help="Show energy vs threshold")
    args = parser.parse_args()

    # â”€â”€ List devices â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    devices = sd.query_devices()
    inputs  = [(i,d) for i,d in enumerate(devices) if d["max_input_channels"]>0]
    outputs = [(i,d) for i,d in enumerate(devices) if d["max_output_channels"]>0]
    if args.list_devices:
        print("Input devices:")
        for i,d in inputs:
            print(f"[{i}] {d['name']}")
        return
    if args.list_output_devices:
        print("Output devices:")
        for i,d in outputs:
            print(f"[{i}] {d['name']}")
        return

    # â”€â”€ Pick mic & speaker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    mic_index = args.mic if args.mic is not None else inputs[0][0]
    out_index = args.output_device if args.output_device is not None else sd.default.device[1]
    print(f"âœ”ï¸ Using mic #{mic_index}:    {devices[mic_index]['name']}")
    print(f"âœ”ï¸ Using speaker #{out_index}: {devices[out_index]['name']}")

    api_key = os.getenv("OPENAI_API_KEY") or sys.exit("OPENAI_API_KEY not set")

    loop               = asyncio.get_running_loop()
    speaking           = Flag()      # assistant audio playing
    awaiting_response  = Flag()      # waiting on assistant
    send_lock          = asyncio.Lock()
    assistant_done_evt = asyncio.Event()

    # â”€â”€ PCM output stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    out_stream = sd.RawOutputStream(
        samplerate=OUTPUT_SR,
        channels=1,
        dtype="int16",
        blocksize=FRAME_SAMPLES,
        device=out_index,
    )
    out_stream.start()

    def play_audio_safe(pcm: bytes) -> None:
        try:
            out_stream.write(pcm)
        except:
            pass

    # â”€â”€ Energy debug â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def debug_energy(e: float, thr: float):
        print(f"\rEnergy {e:.4f} thr {thr:.4f}", end="", flush=True)

    # â”€â”€ Event handlers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _on_audio_start(*_):
        speaking.set(True)
    def _on_audio_end(*_):
        speaking.set(False)
        if awaiting_response.get():
            awaiting_response.set(False)
            assistant_done_evt.set()
            print("\nðŸ‘‚ Your turn â€” listening now")
    def _on_response_done(*_):
        awaiting_response.set(False)
        assistant_done_evt.set()
        print("\nðŸ‘‚ Your turn â€” listening now")
    def _on_interrupted(*_):
        speaking.set(False)
        awaiting_response.set(False)
        assistant_done_evt.set()
        print("\nðŸ‘‚ Your turn â€” listening now")

    # â”€â”€ Connect to Realtime API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    client = RealtimeClient(
        api_key              = api_key,
        instructions         = SYSTEM_PROMPT,
        voice                = "alloy",
        turn_detection_mode  = TurnDetectionMode.MANUAL,
        on_text_delta        = lambda t: print(f"\nAssistant: {t}", end="", flush=True),
        on_audio_delta       = play_audio_safe,
        extra_event_handlers = {
            "response.audio.delta":  _on_audio_start,
            "response.audio.done":   _on_audio_end,
            "response.done":         _on_response_done,
            "response.interrupted":  _on_interrupted,
        }
    )

    keyboard.Listener(on_press=lambda k: loop.stop() if getattr(k,"char","")=="q" else None).start()
    await client.connect()
    asyncio.create_task(client.handle_messages())

    # â”€â”€ Greeting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ðŸŽ™ï¸ Connected â€“ playing greeting â€¦")
    assistant_done_evt.clear()
    awaiting_response.set(True)
    async with send_lock:
        await client.create_response()
    await assistant_done_evt.wait()
    print("ðŸ‘‚ Your turn â€” listening now")

    # â”€â”€ Static VAD calibration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if HAVE_WEBRTC:
        vad = webrtcvad.Vad(3)  # Aggressiveness mode 3
        print("[INFO] WebRTC VAD enabled, mode=3")
    else:
        print("[INFO] energy VAD")
        print(f"ðŸ”‡ Calibrating {CALIBRATE_SEC}s ambientâ€¦")
        amb = sd.rec(int(CALIBRATE_SEC*SAMPLE_RATE),
                     samplerate=SAMPLE_RATE,
                     channels=1,
                     dtype="float32",
                     device=mic_index)
        sd.wait()
        rms = float(np.sqrt((amb[:,0]**2).mean()))
        energy_threshold = rms*ENERGY_MULT + ENERGY_OFFSET
        print(f"âœ… Threshold: {energy_threshold:.6f}")

    # â”€â”€ Speech capture, frame-based hangover â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    in_speech     = False
    voice_frames  = 0
    silence_frames = 0
    buffer        = bytearray()

    async def send_utt(data: bytes) -> None:
        assistant_done_evt.clear()
        awaiting_response.set(True)
        async with send_lock:
            try:
                await client.stream_audio(data)
                await client.create_response()
            finally:
                await assistant_done_evt.wait()

    def callback(indata, *_):
        nonlocal in_speech, voice_frames, silence_frames, buffer

        if speaking.get() or awaiting_response.get():
            return

        samples = indata[:,0]
        energy  = float(np.sqrt((samples**2).mean()))
        pcm     = (samples * 32767).astype('int16').tobytes()

        # debug energy (only in fallback)
        if args.debug_level=="energy" and not HAVE_WEBRTC:
            debug_energy(energy, energy_threshold)

        # VAD decision
        if HAVE_WEBRTC:
            is_voice = vad.is_speech(pcm, SAMPLE_RATE)
        else:
            is_voice = energy > energy_threshold

        # mic status indicator
        status = "ðŸ”Š" if is_voice else "ðŸ”ˆ"
        print(f"\r[{status}]", end="", flush=True)

        if is_voice:
            voice_frames += 1
            silence_frames = 0
            if not in_speech and voice_frames >= START_SPEECH_FRAMES:
                in_speech = True
                buffer.clear()
                print("\n[ðŸŽ¤ Recording â€¦]")
            if in_speech:
                buffer.extend(pcm)
        else:
            silence_frames += 1
            if not in_speech:
                voice_frames = 0
            elif silence_frames < END_SILENCE_FRAMES:
                buffer.extend(pcm)
            else:
                # end of utterance
                in_speech = False
                print("\n[Speech ended]")
                if len(buffer)//2//FRAME_SAMPLES >= MIN_UTTERANCE_FRAMES:
                    asyncio.run_coroutine_threadsafe(send_utt(bytes(buffer)), loop)
                buffer.clear()
                voice_frames = silence_frames = 0

    with sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype="float32",
        blocksize=FRAME_SAMPLES,
        device=mic_index,
        callback=callback,
    ):
        print("\nðŸ”´ Speak when ready (press q to quit) â€¦")
        try:
            await asyncio.Future()
        except asyncio.CancelledError:
            pass

    out_stream.stop()
    out_stream.close()
    await client.close()
    print("\nðŸ‘‹ Bye")

if __name__=="__main__":
    try:
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        pass
